{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 61\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_sample = 10000\n",
    "data_path = './cmn-eng/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "for line in lines[:min(len(lines)-1, num_sample)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for c in input_text:\n",
    "        if c not in input_chars:\n",
    "            input_chars.add(c)\n",
    "            \n",
    "    for c in target_text:\n",
    "        if c not in target_chars:\n",
    "            target_chars.add(c)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2577\n",
      "Max sequence length for inputs: 31\n",
      "Max sequence length for outputs: 23\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_chars)\n",
    "num_decoder_tokens = len(target_chars)\n",
    "max_encoder_seq_length = max([len(l) for l in input_texts]) + 1\n",
    "max_decoder_seq_length = max([len(l) for l in target_texts]) + 1\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = {c: i for i, c in enumerate(input_chars)}\n",
    "input_index_token = {i: c for i, c in enumerate(input_chars)}\n",
    "\n",
    "target_token_index = {c: i for i, c in enumerate(target_chars)}\n",
    "target_index_token = {i: c for i, c in enumerate(target_chars)}\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for j, c in enumerate(input_text):\n",
    "        encoder_input_data[i, j, input_token_index[c]] = 1.\n",
    "    \n",
    "    encoder_input_data[i, j+1:, input_token_index[' ']] = 1.\n",
    "        \n",
    "    # h  e l l o \\n \n",
    "    # \\t h e l l o \\n\n",
    "    for j, c in enumerate(target_text):\n",
    "        decoder_input_data[i, j, target_token_index[c]] = 1.\n",
    "        \n",
    "        if j > 0:\n",
    "            decoder_target_data[i, j-1, target_token_index[c]] = 1.\n",
    "        \n",
    "    decoder_input_data[i, j+1, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, j, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t嗨。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t你好。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t你用跑的。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t等等！\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t你好。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "嗨。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "你好。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "你用跑的。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "等等！\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "你好。\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_text(input_data):\n",
    "    for i in range(0, 5):\n",
    "        input_text = ''\n",
    "    \n",
    "        for j in input_data[i]:\n",
    "            pos = np.argmax(j)\n",
    "            input_text += target_index_token[pos]\n",
    "            \n",
    "        print(input_text)\n",
    "\n",
    "reconstruct_text(decoder_input_data)    \n",
    "reconstruct_text(decoder_target_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 2577)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 337920      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  2902016     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2577)   662289      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,902,225\n",
      "Trainable params: 3,902,225\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 123s 15ms/sample - loss: 1.9738 - accuracy: 0.0836 - val_loss: 2.4198 - val_accuracy: 0.1223\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 115s 14ms/sample - loss: 1.7741 - accuracy: 0.1328 - val_loss: 2.2856 - val_accuracy: 0.1415\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 114s 14ms/sample - loss: 1.6598 - accuracy: 0.1468 - val_loss: 2.2212 - val_accuracy: 0.1467\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 113s 14ms/sample - loss: 1.5916 - accuracy: 0.1561 - val_loss: 2.1196 - val_accuracy: 0.1648\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 1.5082 - accuracy: 0.1660 - val_loss: 2.1044 - val_accuracy: 0.1607\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.4386 - accuracy: 0.1760 - val_loss: 2.0011 - val_accuracy: 0.1833\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.3771 - accuracy: 0.1838 - val_loss: 1.9807 - val_accuracy: 0.1807\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.3266 - accuracy: 0.1888 - val_loss: 1.9472 - val_accuracy: 0.1903\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.2787 - accuracy: 0.1938 - val_loss: 1.9246 - val_accuracy: 0.1946\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 1.2336 - accuracy: 0.1998 - val_loss: 1.9055 - val_accuracy: 0.1919\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.1929 - accuracy: 0.2039 - val_loss: 1.8612 - val_accuracy: 0.1982\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.1557 - accuracy: 0.2084 - val_loss: 1.8069 - val_accuracy: 0.2092\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 1.1181 - accuracy: 0.2128 - val_loss: 1.7991 - val_accuracy: 0.2113\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.0853 - accuracy: 0.2171 - val_loss: 1.7889 - val_accuracy: 0.2106\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 1.0540 - accuracy: 0.2204 - val_loss: 1.7644 - val_accuracy: 0.2118\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 1.0249 - accuracy: 0.2240 - val_loss: 1.7542 - val_accuracy: 0.2171\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 113s 14ms/sample - loss: 0.9965 - accuracy: 0.2273 - val_loss: 1.7633 - val_accuracy: 0.2165\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 117s 15ms/sample - loss: 0.9698 - accuracy: 0.2306 - val_loss: 1.7525 - val_accuracy: 0.2166\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 163s 20ms/sample - loss: 0.9446 - accuracy: 0.2344 - val_loss: 1.7676 - val_accuracy: 0.2131\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.9204 - accuracy: 0.2374 - val_loss: 1.7417 - val_accuracy: 0.2193\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.8968 - accuracy: 0.2412 - val_loss: 1.7473 - val_accuracy: 0.2218\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.8750 - accuracy: 0.2441 - val_loss: 1.7396 - val_accuracy: 0.2214\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.8532 - accuracy: 0.2466 - val_loss: 1.7288 - val_accuracy: 0.2213\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.8320 - accuracy: 0.2505 - val_loss: 1.7210 - val_accuracy: 0.2214\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.8122 - accuracy: 0.2539 - val_loss: 1.7340 - val_accuracy: 0.2199\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.7927 - accuracy: 0.2563 - val_loss: 1.7323 - val_accuracy: 0.2221\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.7736 - accuracy: 0.2593 - val_loss: 1.7223 - val_accuracy: 0.2237\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 130s 16ms/sample - loss: 0.7548 - accuracy: 0.2628 - val_loss: 1.7482 - val_accuracy: 0.2212\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.7368 - accuracy: 0.2659 - val_loss: 1.7347 - val_accuracy: 0.2241\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.7192 - accuracy: 0.2685 - val_loss: 1.7431 - val_accuracy: 0.2238\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.7026 - accuracy: 0.2717 - val_loss: 1.7601 - val_accuracy: 0.2227\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.6855 - accuracy: 0.2751 - val_loss: 1.7533 - val_accuracy: 0.2235\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.6703 - accuracy: 0.2777 - val_loss: 1.7513 - val_accuracy: 0.2250\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.6538 - accuracy: 0.2806 - val_loss: 1.7664 - val_accuracy: 0.2250\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.6387 - accuracy: 0.2828 - val_loss: 1.7618 - val_accuracy: 0.2243\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.6234 - accuracy: 0.2862 - val_loss: 1.7677 - val_accuracy: 0.2238\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.6089 - accuracy: 0.2892 - val_loss: 1.7771 - val_accuracy: 0.2234\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.5943 - accuracy: 0.2920 - val_loss: 1.7945 - val_accuracy: 0.2225\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 150s 19ms/sample - loss: 0.5813 - accuracy: 0.2943 - val_loss: 1.7726 - val_accuracy: 0.2263\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.5673 - accuracy: 0.2971 - val_loss: 1.8029 - val_accuracy: 0.2252\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.5546 - accuracy: 0.2995 - val_loss: 1.7888 - val_accuracy: 0.2265\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.5423 - accuracy: 0.3019 - val_loss: 1.8116 - val_accuracy: 0.2245\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.5300 - accuracy: 0.3041 - val_loss: 1.8207 - val_accuracy: 0.2232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 110s 14ms/sample - loss: 0.5186 - accuracy: 0.3065 - val_loss: 1.7980 - val_accuracy: 0.2263\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.5059 - accuracy: 0.3086 - val_loss: 1.8271 - val_accuracy: 0.2224\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 113s 14ms/sample - loss: 0.4952 - accuracy: 0.3111 - val_loss: 1.8286 - val_accuracy: 0.2248\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 110s 14ms/sample - loss: 0.4845 - accuracy: 0.3134 - val_loss: 1.8436 - val_accuracy: 0.2262\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.4736 - accuracy: 0.3149 - val_loss: 1.8472 - val_accuracy: 0.2232\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.4634 - accuracy: 0.3174 - val_loss: 1.8432 - val_accuracy: 0.2258\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 130s 16ms/sample - loss: 0.4529 - accuracy: 0.3189 - val_loss: 1.8609 - val_accuracy: 0.2251\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.4429 - accuracy: 0.3211 - val_loss: 1.8481 - val_accuracy: 0.2244\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.4331 - accuracy: 0.3232 - val_loss: 1.8625 - val_accuracy: 0.2241\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 160s 20ms/sample - loss: 0.4247 - accuracy: 0.3250 - val_loss: 1.8668 - val_accuracy: 0.2247\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.4148 - accuracy: 0.3269 - val_loss: 1.8701 - val_accuracy: 0.2248\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.4098 - accuracy: 0.3279 - val_loss: 1.8845 - val_accuracy: 0.2260\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.3988 - accuracy: 0.3299 - val_loss: 1.8833 - val_accuracy: 0.2247\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.3898 - accuracy: 0.3322 - val_loss: 1.8865 - val_accuracy: 0.2255\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 163s 20ms/sample - loss: 0.3811 - accuracy: 0.3337 - val_loss: 1.8983 - val_accuracy: 0.2251\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 163s 20ms/sample - loss: 0.3732 - accuracy: 0.3349 - val_loss: 1.8954 - val_accuracy: 0.2241\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.3772 - accuracy: 0.3342 - val_loss: 1.9155 - val_accuracy: 0.2253\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.3587 - accuracy: 0.3375 - val_loss: 1.9156 - val_accuracy: 0.2264\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.3509 - accuracy: 0.3394 - val_loss: 1.9160 - val_accuracy: 0.2255\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 150s 19ms/sample - loss: 0.3439 - accuracy: 0.3408 - val_loss: 1.9377 - val_accuracy: 0.2249\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.3367 - accuracy: 0.3421 - val_loss: 1.9287 - val_accuracy: 0.2260\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.3298 - accuracy: 0.3432 - val_loss: 1.9336 - val_accuracy: 0.2246\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.3226 - accuracy: 0.3448 - val_loss: 1.9456 - val_accuracy: 0.2233\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.3160 - accuracy: 0.3462 - val_loss: 1.9529 - val_accuracy: 0.2256\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.3094 - accuracy: 0.3474 - val_loss: 1.9597 - val_accuracy: 0.2262\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.3029 - accuracy: 0.3485 - val_loss: 1.9685 - val_accuracy: 0.2225\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2968 - accuracy: 0.3499 - val_loss: 1.9668 - val_accuracy: 0.2255\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2901 - accuracy: 0.3512 - val_loss: 1.9737 - val_accuracy: 0.2251\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2843 - accuracy: 0.3521 - val_loss: 1.9897 - val_accuracy: 0.2215\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2778 - accuracy: 0.3533 - val_loss: 2.0044 - val_accuracy: 0.2247\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2721 - accuracy: 0.3546 - val_loss: 1.9933 - val_accuracy: 0.2248\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2669 - accuracy: 0.3558 - val_loss: 2.0087 - val_accuracy: 0.2262\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2606 - accuracy: 0.3567 - val_loss: 2.0075 - val_accuracy: 0.2254\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2542 - accuracy: 0.3581 - val_loss: 2.0291 - val_accuracy: 0.2236\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2496 - accuracy: 0.3590 - val_loss: 2.0244 - val_accuracy: 0.2229\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2438 - accuracy: 0.3601 - val_loss: 2.0321 - val_accuracy: 0.2246\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 117s 15ms/sample - loss: 0.2381 - accuracy: 0.3613 - val_loss: 2.0547 - val_accuracy: 0.2225\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 163s 20ms/sample - loss: 0.2327 - accuracy: 0.3623 - val_loss: 2.0348 - val_accuracy: 0.2246\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 123s 15ms/sample - loss: 0.2286 - accuracy: 0.3631 - val_loss: 2.0345 - val_accuracy: 0.2232\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2228 - accuracy: 0.3645 - val_loss: 2.0667 - val_accuracy: 0.2213\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2177 - accuracy: 0.3654 - val_loss: 2.0543 - val_accuracy: 0.2234\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2129 - accuracy: 0.3664 - val_loss: 2.0486 - val_accuracy: 0.2236\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.2076 - accuracy: 0.3675 - val_loss: 2.0604 - val_accuracy: 0.2225\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.2032 - accuracy: 0.3684 - val_loss: 2.0862 - val_accuracy: 0.2221\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.1984 - accuracy: 0.3694 - val_loss: 2.0872 - val_accuracy: 0.2202\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.1944 - accuracy: 0.3700 - val_loss: 2.0995 - val_accuracy: 0.2200\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.1892 - accuracy: 0.3715 - val_loss: 2.0836 - val_accuracy: 0.2230\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 112s 14ms/sample - loss: 0.1847 - accuracy: 0.3720 - val_loss: 2.1169 - val_accuracy: 0.2188\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 111s 14ms/sample - loss: 0.1805 - accuracy: 0.3732 - val_loss: 2.1140 - val_accuracy: 0.2237\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 138s 17ms/sample - loss: 0.1762 - accuracy: 0.3743 - val_loss: 2.1183 - val_accuracy: 0.2220\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 164s 21ms/sample - loss: 0.1719 - accuracy: 0.3752 - val_loss: 2.1144 - val_accuracy: 0.2217\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.1670 - accuracy: 0.3764 - val_loss: 2.1325 - val_accuracy: 0.2222\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.1635 - accuracy: 0.3771 - val_loss: 2.1423 - val_accuracy: 0.2201\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.1596 - accuracy: 0.3779 - val_loss: 2.1400 - val_accuracy: 0.2213\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.1553 - accuracy: 0.3786 - val_loss: 2.1388 - val_accuracy: 0.2226\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 162s 20ms/sample - loss: 0.1517 - accuracy: 0.3796 - val_loss: 2.1601 - val_accuracy: 0.2205\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 161s 20ms/sample - loss: 0.1471 - accuracy: 0.3807 - val_loss: 2.1559 - val_accuracy: 0.2215\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 73)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 337920    \n",
      "=================================================================\n",
      "Total params: 337,920\n",
      "Trainable params: 337,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 2577)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  2902016     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2577)   662289      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,564,305\n",
      "Trainable params: 3,564,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "# ENCODER\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "# DECODER\n",
    "decoder_state_inputs = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\n",
    "decoder_outputs, next_h, next_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "next_states = [next_h, next_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_inputs,\n",
    "    [decoder_outputs] + next_states)\n",
    "\n",
    "decoder_model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 你用跑的。\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 等等！\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 让我来。\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我赢了。\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 不会吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 嗨！\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: 你懂了吗？\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 他跑了。\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 跳进来。\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我迷失了。\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我迷失了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我沒事。\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 听着。\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 没门！\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 没门！\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 你确定？\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 试试吧。\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我们来试试。\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 为什么是我？\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 去问汤姆。\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: 好棒！\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 冷静点。\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 公平点。\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 友善点。\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 和气点。\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 联系我。\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 联系我们。\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 来加入我们吧。\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 出去。\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 滾出去！\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 滾出去！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 告辞！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 告辞！\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: 等一下！\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: 他来了。\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: 他跑。\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: 帮我一下。\n",
      "\n",
      "-\n",
      "Input sentence: Help us.\n",
      "Decoded sentence: 帮帮我们吧！\n",
      "\n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: 停火。\n",
      "\n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: 抱抱汤姆！\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: 我同意。\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: 我很快。\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: 是個。\n",
      "\n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: 這是我份的。\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: 来加入我们吧。\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: 保持安静\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: 我是女朋友。\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: 请开！\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: 再见！\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: 和我们这个。\n",
      "\n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: 不管它。\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: 拿走吧。\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: 醒醒！\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: 去清洗一下。\n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: 我們知道她。\n",
      "\n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: 欢迎。\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: 誰赢了？\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: 为什么不？\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: 你跑。\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: 请跟着我们。\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: 放开他。\n",
      "\n",
      "-\n",
      "Input sentence: Beats me.\n",
      "Decoded sentence: 我一无所知。\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: 把他铐上。\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: 开心！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: 抓住汤姆。\n",
      "\n",
      "-\n",
      "Input sentence: Good job!\n",
      "Decoded sentence: 做得好！\n",
      "\n",
      "-\n",
      "Input sentence: Good job!\n",
      "Decoded sentence: 做得好！\n",
      "\n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: 抓住汤姆。\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: 抓住他。\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: 玩得開心。\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: 他来试试。\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: 你就随了我的意吧。\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快点！\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快点！\n",
      "\n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: 我忘了。\n",
      "\n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: 我放弃。\n",
      "\n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: 我來付錢。\n",
      "\n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: 我很忙。\n",
      "\n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: 我冷。\n",
      "\n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm tall.\n",
      "Decoded sentence: 我个子高。\n",
      "\n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: 让我一个人呆会儿。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們開始吧！\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們開始吧！\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們開始吧！\n",
      "\n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: 当心！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    stop = False\n",
    "    decoded_text = ''\n",
    "    while not stop:\n",
    "        o, h, c = decoder_model.predict([target_seq] + states)\n",
    "        \n",
    "        sampled_token_index = np.argmax(o[0,-1,:])\n",
    "        sampled_char = target_index_token[sampled_token_index]\n",
    "        decoded_text += sampled_char\n",
    "        \n",
    "        if sampled_char == '\\n' or len(decoded_text) > max_decoder_seq_length:\n",
    "            stop = True\n",
    "            \n",
    "        target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        states = [h, c]\n",
    "        \n",
    "    return decoded_text\n",
    "        \n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
